# -*- coding: utf-8 -*-
"""data scraping

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yFn0ABpQoftIzYbMho5S4RNkhCHl7kwc
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import time # For adding delays
import json # For handling potential JSON API responses

def scrape_nifty_indices():

    url = "https://www.niftyindices.com/market-data/live-index-watch"

    # Headers to mimic a web browser request, which can help avoid being blocked
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/" # Can sometimes help with anti-scraping measures
    }

    print(f"Attempting to scrape data from: {url}")

    try:
        # Step 1: Attempt to fetch the page content
        response = requests.get(url, headers=headers, timeout=10) # Add a timeout
        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)

        try:
            tables = pd.read_html(response.text)
            if tables:
                print(f"Found {len(tables)} HTML tables on the page.")

                for i, df in enumerate(tables):
                    if 'Index Name' in df.columns or 'Last Price' in df.columns:
                        print(f"--- Data from Table {i} (Likely Relevant) ---")
                        print(df.head())
                        df['Scrape_Timestamp'] = datetime.datetime.now().isoformat()
                        return df
                print("No obvious index table found using pandas.read_html. Proceeding with BeautifulSoup.")

        except ValueError:
            print("No tables found by pandas.read_html or tables are not well-formed.")

        soup = BeautifulSoup(response.content, 'html.parser')

        indices_data = []

        container = soup.find('div', class_='live-indices-container') # Replace with actual container class/ID
        if container:
            index_items = container.find_all('div', class_='index-item') # Replace with actual item class/tag
            for item in index_items:
                name_element = item.find('span', class_='index-name') # Replace with actual name class/tag
                price_element = item.find('span', class_='current-price') # Replace with actual price class/tag
                change_element = item.find('span', class_='change-value') # Replace with actual change class/tag
                percent_change_element = item.find('span', class_='percent-change') # Replace with actual percent change class/tag

                name = name_element.text.strip() if name_element else 'N/A'
                price = price_element.text.strip() if price_element else 'N/A'
                change = change_element.text.strip() if change_element else 'N/A'
                percent_change = percent_change_element.text.strip() if percent_change_element else 'N/A'

                indices_data.append({
                    'Index Name': name,
                    'Last Price': price,
                    'Change': change,
                    'Percent Change': percent_change,
                    'Scrape_Timestamp': datetime.datetime.now().isoformat()
                })

        if indices_data:
            df = pd.DataFrame(indices_data)
            print("--- Data Extracted using BeautifulSoup ---")
            print(df.head())
            return df
        else:
            print("No specific index data found using BeautifulSoup selectors. This might mean:")
            print("  1. The HTML structure has changed.")
            print("  2. The data is loaded dynamically via JavaScript (API calls).")
            print("Please inspect the network tab in your browser's developer tools for API calls.")

            return pd.DataFrame() # Return empty DataFrame if nothing found

    except requests.exceptions.RequestException as e:
        print(f"Network or HTTP error fetching the page: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during scraping: {e}")
        return None

# --- How to use this script for your data stream ---

def run_daily_scrape_and_store():
    """
    Function to be called daily by a scheduler.
    Scrapes data and stores it (e.g., appends to a CSV file).
    """
    print(f"\n--- Running daily scrape at {datetime.datetime.now()} ---")
    scraped_df = scrape_nifty_indices()

    if scraped_df is not None and not scraped_df.empty:
        # Define your output file path
        output_csv_path = "nifty_indices_data_stream.csv"

        # Check if file exists to write header only once
        try:
            with open(output_csv_path, 'x') as f: # 'x' mode creates file, errors if exists
                scraped_df.to_csv(f, index=False, header=True)
            print(f"Created new CSV file: {output_csv_path} and added header.")
        except FileExistsError:
            # File already exists, append data without header
            scraped_df.to_csv(output_csv_path, mode='a', index=False, header=False)
            print(f"Appended data to existing CSV file: {output_csv_path}")

        print("Data successfully scraped and stored.")
    else:
        print("Scraping failed or returned no data. Nothing to store.")

    # Add a delay to be polite to the server, especially if running frequently
    time.sleep(5) # Wait for 5 seconds before exiting (if this were a single run)

# --- Uncomment the line below to test the scraping function ---
run_daily_scrape_and_store()