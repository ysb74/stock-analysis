# -*- coding: utf-8 -*-
"""DNN Trend Prediction and Anomaly Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pHKixHab9QiCBO_VxCW0URHwzw2dAY85
"""

import pandas as pd
import numpy as np
import os
import datetime
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, r2_score
import warnings

# Suppress specific warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# --- Configuration ---
SCRAPED_DATA_DIR = "nifty_daily_data_csv" # Directory where your daily CSVs are saved
TARGET_COLUMN = 'Last Price' # The column in your scraped data to predict
PREDICTION_HORIZON_24HR = 1 # Predict 1 day ahead
PREDICTION_HORIZON_MONTH = 30 # Predict 30 days ahead (approx. 1 month)

# --- Data Loading Function ---
def load_historical_data(directory):
    """
    Loads all CSV files from the specified directory into a single DataFrame.
    Assumes CSVs are named with timestamps and contain the same columns.
    """
    all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]

    if not all_files:
        print(f"[{datetime.datetime.now()}] No CSV files found in '{directory}'. Please ensure the scraper has run and saved data.")
        return pd.DataFrame()

    df_list = []
    for f in sorted(all_files): # Sort to ensure chronological order
        try:
            df = pd.read_csv(f)
            # Ensure 'Scrape_Timestamp' is parsed as datetime
            if 'Scrape_Timestamp' in df.columns:
                df['Scrape_Timestamp'] = pd.to_datetime(df['Scrape_Timestamp'])
            else:
                # Fallback if timestamp column is missing, try to infer from filename
                try:
                    ts_str = os.path.basename(f).split('_')[2].split('.')[0]
                    df['Scrape_Timestamp'] = pd.to_datetime(ts_str, format='%H%M%S') # Adjust format if needed
                except IndexError:
                    print(f"[{datetime.datetime.now()}] Warning: Could not infer timestamp from filename for {f}. Skipping.")
                    continue
            df_list.append(df)
        except Exception as e:
            print(f"[{datetime.datetime.now()}] Error loading {f}: {e}")
            continue

    if not df_list:
        return pd.DataFrame()

    combined_df = pd.concat(df_list, ignore_index=True)

    # Sort by timestamp to ensure correct time series order
    if 'Scrape_Timestamp' in combined_df.columns:
        combined_df = combined_df.sort_values(by='Scrape_Timestamp').drop_duplicates(subset=['Scrape_Timestamp', 'Index Name'], keep='last')

    print(f"[{datetime.datetime.now()}] Loaded {len(combined_df)} records from {len(all_files)} files.")
    return combined_df

# --- Data Preprocessing and Feature Engineering ---
def preprocess_data(df, index_name="NIFTY 50"):
    """
    Preprocesses the data for a specific index, creates lagged features,
    and time-based features.
    """
    if df.empty:
        return pd.DataFrame()

    # Filter for the specific index (e.g., NIFTY 50)
    df_index = df[df['Index Name'] == index_name].copy()

    if df_index.empty:
        print(f"[{datetime.datetime.now()}] No data found for index: {index_name}. Check 'Index Name' column in your CSVs.")
        return pd.DataFrame()

    # Ensure 'Scrape_Timestamp' is datetime and set as index
    df_index['Scrape_Timestamp'] = pd.to_datetime(df_index['Scrape_Timestamp'])
    df_index = df_index.set_index('Scrape_Timestamp').sort_index()

    # Convert 'Last Price' to numeric, handling potential errors
    df_index[TARGET_COLUMN] = pd.to_numeric(df_index[TARGET_COLUMN], errors='coerce')
    df_index = df_index.dropna(subset=[TARGET_COLUMN])

    if df_index.empty:
        print(f"[{datetime.datetime.now()}] No valid numeric '{TARGET_COLUMN}' data after cleaning for {index_name}.")
        return pd.DataFrame()

    # Create lagged features (previous day's prices)
    for i in range(1, 6): # Lag by up to 5 days
        df_index[f'Lag_{i}_Price'] = df_index[TARGET_COLUMN].shift(i)

    # Create rolling window features (e.g., 5-day rolling mean)
    df_index['Rolling_Mean_5'] = df_index[TARGET_COLUMN].rolling(window=5).mean().shift(1)
    df_index['Rolling_Std_5'] = df_index[TARGET_COLUMN].rolling(window=5).std().shift(1)

    # Create time-based features
    df_index['Day_of_Week'] = df_index.index.dayofweek
    df_index['Day_of_Month'] = df_index.index.day
    df_index['Month'] = df_index.index.month
    df_index['Year'] = df_index.index.year

    # Drop rows with NaN values created by shifting/rolling (first few rows)
    df_index = df_index.dropna()

    print(f"[{datetime.datetime.now()}] Processed data for {index_name}. Shape: {df_index.shape}")
    return df_index

# --- DNN Model Training Function ---
def train_dnn_model(df_processed):
    """
    Trains a Deep Neural Network (DNN) for price prediction.
    """
    if df_processed.empty:
        print(f"[{datetime.datetime.now()}] Processed DataFrame is empty. Cannot train DNN model.")
        return None, None

    # Define features (X) and target (y)
    # Ensure all features created in preprocessing are included
    features_to_drop = [TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change']
    X = df_processed.drop(columns=[col for col in features_to_drop if col in df_processed.columns], errors='ignore')
    y = df_processed[TARGET_COLUMN]

    # Align X and y indices
    X, y = X.align(y, join='inner', axis=0)

    if X.empty or y.empty:
        print(f"[{datetime.datetime.now()}] Features or target are empty after alignment. Cannot train DNN model.")
        return None, None

    # Scale features
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)

    # Scale target (for better DNN performance, especially with large price ranges)
    scaler_y = MinMaxScaler()
    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)) # Reshape for scaler

    # Split data for training and testing (time-series split)
    # Train on older data, test on newer data
    train_size = int(len(X_scaled) * 0.8)
    X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]
    y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]

    # Keep original y_test for evaluation after inverse scaling
    y_test_original = y.iloc[train_size:]

    print(f"[{datetime.datetime.now()}] Training data size: {len(X_train)} | Test data size: {len(X_test)}")
    print(f"[{datetime.datetime.now()}] Number of features: {X_train.shape[1]}")

    # Build the DNN model
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)), # Input layer
        Dropout(0.2), # Dropout for regularization
        Dense(32, activation='relu'), # Hidden layer
        Dropout(0.2),
        Dense(1) # Output layer for regression (single value prediction)
    ])

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

    # Train the model
    print(f"[{datetime.datetime.now()}] Training DNN model...")
    history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0, validation_split=0.1) # verbose=0 to suppress output
    print(f"[{datetime.datetime.now()}] DNN model training complete.")

    # Evaluate the model on the test set
    y_pred_scaled = model.predict(X_test)
    y_pred_original = scaler_y.inverse_transform(y_pred_scaled)

    mse = mean_squared_error(y_test_original, y_pred_original)
    r2 = r2_score(y_test_original, y_pred_original)
    print(f"[{datetime.datetime.now()}] DNN Model Evaluation - MSE: {mse:.2f}, R2: {r2:.2f}")

    return model, scaler_X, scaler_y # Return model and scalers for future predictions

# --- Prediction Function with DNN ---
def predict_trends_dnn(dnn_model, scaler_X, scaler_y, df_processed):
    """
    Predicts future market trends for 24 hours and 1 month using the trained DNN.
    """
    if dnn_model is None or df_processed.empty:
        print(f"[{datetime.datetime.now()}] DNN model or processed data not available for prediction.")
        return None

    print(f"\n[{datetime.datetime.now()}] --- Making Future Predictions with DNN ---")

    last_known_data = df_processed.iloc[-1]

    # --- Construct features for the next 24 hours (1 day) ---
    future_features_24hr = {}
    for i in range(1, 6): # Lag by up to 5 days
        future_features_24hr[f'Lag_{i}_Price'] = last_known_data[f'Lag_{i-1}_Price'] if i > 1 else last_known_data[TARGET_COLUMN]

    future_features_24hr['Rolling_Mean_5'] = df_processed[TARGET_COLUMN].iloc[-5:].mean()
    future_features_24hr['Rolling_Std_5'] = df_processed[TARGET_COLUMN].iloc[-5:].std()

    next_day_timestamp = last_known_data.name + datetime.timedelta(days=1)
    future_features_24hr['Day_of_Week'] = next_day_timestamp.dayofweek
    future_features_24hr['Day_of_Month'] = next_day_timestamp.day
    future_features_24hr['Month'] = next_day_timestamp.month
    future_features_24hr['Year'] = next_day_timestamp.year

    # Convert to DataFrame for scaling and prediction
    X_future_24hr_df = pd.DataFrame([future_features_24hr])

    # Ensure columns match training data features order
    training_cols = df_processed.drop(columns=[TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change'], errors='ignore').columns
    X_future_24hr_df = X_future_24hr_df.reindex(columns=training_cols, fill_value=0) # Fill missing with 0 or appropriate value

    # Scale the future features
    X_future_24hr_scaled = scaler_X.transform(X_future_24hr_df)

    # Predict for 24 hours
    pred_24hr_scaled = dnn_model.predict(X_future_24hr_scaled)[0][0]
    pred_24hr_original = scaler_y.inverse_transform([[pred_24hr_scaled]])[0][0]

    print(f"[{datetime.datetime.now()}] Next 24 hours (1 day) prediction (DNN): {pred_24hr_original:.2f}")

    # --- Predict for 1 Month (30 days) ---
    # This involves iterative prediction: predict one day, use it as input for the next.
    # This can accumulate errors.

    current_features_df = X_future_24hr_df.copy()
    current_predicted_price = pred_24hr_original

    predictions_month = []

    for i in range(PREDICTION_HORIZON_MONTH):
        # Update lagged features for the next iteration based on the *predicted* price
        for j in range(5, 1, -1):
            current_features_df.loc[0, f'Lag_{j}_Price'] = current_features_df.loc[0, f'Lag_{j-1}_Price']
        current_features_df.loc[0, 'Lag_1_Price'] = current_predicted_price # The new predicted price becomes Lag_1

        # Update rolling features (simplified: just use the predicted price for mean/std approximation)
        # For more accuracy, you'd need to maintain a rolling window of actual/predicted values.
        # This is a significant simplification for a month-long forecast.
        # A more robust approach would dynamically update the rolling mean/std based on the new predicted price.
        # For now, we'll keep it simple to avoid complex rolling window updates in a loop.

        # Update time-based features for the next day in the sequence
        current_timestamp = next_day_timestamp + datetime.timedelta(days=i)
        current_features_df.loc[0, 'Day_of_Week'] = current_timestamp.dayofweek
        current_features_df.loc[0, 'Day_of_Month'] = current_timestamp.day
        current_features_df.loc[0, 'Month'] = current_timestamp.month
        current_features_df.loc[0, 'Year'] = current_timestamp.year

        # Scale features for the current iteration
        current_features_scaled = scaler_X.transform(current_features_df)

        # Predict the next day's price
        next_pred_scaled = dnn_model.predict(current_features_scaled)[0][0]
        current_predicted_price = scaler_y.inverse_transform([[next_pred_scaled]])[0][0]

        predictions_month.append(current_predicted_price)

    avg_month_pred = np.mean(predictions_month)
    month_end_pred = predictions_month[-1]

    print(f"[{datetime.datetime.now()}] Next 1 month ({PREDICTION_HORIZON_MONTH} days) predictions (DNN):")
    print(f"  Average over month: {avg_month_pred:.2f}")
    print(f"  Month-end prediction: {month_end_pred:.2f}")

    return {
        '24hr_dnn': pred_24hr_original,
        'month_dnn_avg': avg_month_pred,
        'month_dnn_end': month_end_pred
    }

# --- Anomaly Detection Function with DNN Residuals ---
def detect_anomalies_dnn(dnn_model, scaler_X, scaler_y, df_processed):
    """
    Detects anomalies based on the DNN's prediction residuals.
    Anomalies are points where the actual value deviates significantly from the predicted value.
    """
    if dnn_model is None or df_processed.empty:
        print(f"[{datetime.datetime.now()}] DNN model or processed data not available for anomaly detection.")
        return pd.DataFrame()

    print(f"\n[{datetime.datetime.now()}] --- Detecting Anomalies with DNN Residuals ---")

    # Features for prediction on historical data
    features_to_drop = [TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change']
    X_historical = df_processed.drop(columns=[col for col in features_to_drop if col in df_processed.columns], errors='ignore')
    y_historical = df_processed[TARGET_COLUMN]

    # Scale historical features
    X_historical_scaled = scaler_X.transform(X_historical)

    # Get predictions for historical data
    y_pred_scaled = dnn_model.predict(X_historical_scaled)
    y_pred_original = scaler_y.inverse_transform(y_pred_scaled).flatten() # Flatten to 1D array

    # Calculate residuals (prediction errors)
    residuals = y_historical - y_pred_original
    df_processed['prediction_error'] = residuals
    df_processed['abs_prediction_error'] = np.abs(residuals)

    # Define anomaly threshold based on residuals
    # Common approaches:
    # 1. Using standard deviation of residuals (e.g., > 2 or 3 std deviations)
    # 2. Using quantiles (e.g., top 1% or 5% of absolute errors)

    # Using a percentile threshold for simplicity: top 1% of absolute errors
    error_threshold = df_processed['abs_prediction_error'].quantile(0.99)
    print(f"[{datetime.datetime.now()}] Anomaly detection threshold (absolute error): {error_threshold:.2f}")

    anomalies = df_processed[df_processed['abs_prediction_error'] > error_threshold].copy()
    anomalies['is_anomaly'] = -1 # Consistent with IsolationForest output convention

    if not anomalies.empty:
        print(f"[{datetime.datetime.now()}] Detected {len(anomalies)} anomalies:")
        print(anomalies[[TARGET_COLUMN, 'prediction_error', 'abs_prediction_error', 'is_anomaly']])
    else:
        print(f"[{datetime.datetime.now()}] No anomalies detected based on the current threshold.")

    return anomalies

# --- Main Automation Function for DNN Prediction and Anomaly Detection ---
def run_dnn_market_automation(index_to_analyze="NIFTY 50"):
    """
    Orchestrates the market trend prediction and anomaly detection process using a DNN.
    """
    print(f"\n[{datetime.datetime.now()}] --- Starting DNN Market Trend Prediction and Anomaly Detection ---")
    print(f"Analyzing index: {index_to_analyze}")

    # 1. Load historical data
    combined_data = load_historical_data(SCRAPED_DATA_DIR)
    if combined_data.empty:
        print(f"[{datetime.datetime.now()}] No historical data loaded. Exiting.")
        return

    # 2. Preprocess data and engineer features
    processed_data = preprocess_data(combined_data, index_name=index_to_analyze)
    if processed_data.empty:
        print(f"[{datetime.datetime.now()}] Data preprocessing failed or resulted in empty DataFrame. Exiting.")
        return

    # Ensure we have enough data for lags and training
    if len(processed_data) < 50: # Increased minimum for DNN
        print(f"[{datetime.datetime.now()}] Not enough historical data ({len(processed_data)} records) for meaningful DNN analysis. Need at least 50 records.")
        return

    # 3. Train DNN model
    dnn_model, scaler_X, scaler_y = train_dnn_model(processed_data)
    if dnn_model is None:
        print(f"[{datetime.datetime.now()}] DNN model training failed. Exiting.")
        return

    # 4. Predict future trends with DNN
    predictions = predict_trends_dnn(dnn_model, scaler_X, scaler_y, processed_data)

    # 5. Detect anomalies with DNN residuals
    anomalies_df = detect_anomalies_dnn(dnn_model, scaler_X, scaler_y, processed_data)

    print(f"\n[{datetime.datetime.now()}] --- DNN Market Trend Prediction and Anomaly Detection Finished ---")

    return predictions, anomalies_df

# --- Main execution block ---
if __name__ == "__main__":
    # Example usage:
    # Make sure you have scraped data in the 'nifty_daily_data_csv' directory first!
    # The more data you have (e.g., hundreds of days), the better a DNN will perform.

    predictions, anomalies = run_dnn_market_automation(index_to_analyze="NIFTY 50")

    if predictions:
        print("\nFinal DNN Predictions Summary:")
        for key, value in predictions.items():
            if value is not None:
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: N/A (Prediction failed or not applicable)")

    if anomalies is not None and not anomalies.empty:
        print("\nDetected Anomalies (Full Details):")
        print(anomalies)