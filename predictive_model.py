# -*- coding: utf-8 -*-
"""predictive model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kabu4hLRsCER4wH_Gc8KBUXsRAZxu5q1
"""

import pandas as pd
import numpy as np
import os
import datetime
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge # Simple regression model
from sklearn.ensemble import GradientBoostingRegressor # Ensemble method for regression
from sklearn.ensemble import IsolationForest # Anomaly detection
from statsmodels.tsa.arima.model import ARIMA # Time series specific model
from sklearn.metrics import mean_squared_error, r2_score
import warnings

# Suppress specific warnings from statsmodels
warnings.filterwarnings("ignore", category=UserWarning, module="statsmodels")
warnings.filterwarnings("ignore", category=FutureWarning, module="statsmodels")

# --- Configuration ---
SCRAPED_DATA_DIR = "nifty_daily_data_csv" # Directory where your daily CSVs are saved
TARGET_COLUMN = 'Last Price' # The column in your scraped data to predict
PREDICTION_HORIZON_24HR = 1 # Predict 1 day ahead
PREDICTION_HORIZON_MONTH = 30 # Predict 30 days ahead (approx. 1 month)

# --- Data Loading Function ---
def load_historical_data(directory):
    """
    Loads all CSV files from the specified directory into a single DataFrame.
    Assumes CSVs are named with timestamps and contain the same columns.
    """
    all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]

    if not all_files:
        print(f"No CSV files found in '{directory}'. Please ensure the scraper has run and saved data.")
        return pd.DataFrame()

    df_list = []
    for f in sorted(all_files): # Sort to ensure chronological order
        try:
            df = pd.read_csv(f)
            # Ensure 'Scrape_Timestamp' is parsed as datetime
            if 'Scrape_Timestamp' in df.columns:
                df['Scrape_Timestamp'] = pd.to_datetime(df['Scrape_Timestamp'])
            else:
                # If timestamp column is missing, try to infer from filename or add a placeholder
                print(f"Warning: 'Scrape_Timestamp' not found in {f}. Data might not be ordered correctly.")
                df['Scrape_Timestamp'] = pd.to_datetime(os.path.basename(f).split('_')[2].split('.')[0], format='%H%M%S') # Example if timestamp is in filename
            df_list.append(df)
        except Exception as e:
            print(f"Error loading {f}: {e}")
            continue

    if not df_list:
        return pd.DataFrame()

    combined_df = pd.concat(df_list, ignore_index=True)

    # Sort by timestamp to ensure correct time series order
    if 'Scrape_Timestamp' in combined_df.columns:
        combined_df = combined_df.sort_values(by='Scrape_Timestamp').drop_duplicates(subset=['Scrape_Timestamp', 'Index Name'], keep='last')

    print(f"Loaded {len(combined_df)} records from {len(all_files)} files.")
    return combined_df

# --- Data Preprocessing and Feature Engineering ---
def preprocess_data(df, index_name="NIFTY 50"):
    """
    Preprocesses the data for a specific index, creates lagged features,
    and time-based features.
    """
    if df.empty:
        return pd.DataFrame()

    # Filter for the specific index (e.g., NIFTY 50)
    # You might need to adjust 'Index Name' column based on your scraped data
    df_index = df[df['Index Name'] == index_name].copy()

    if df_index.empty:
        print(f"No data found for index: {index_name}. Check 'Index Name' column in your CSVs.")
        return pd.DataFrame()

    # Ensure 'Scrape_Timestamp' is datetime and set as index
    df_index['Scrape_Timestamp'] = pd.to_datetime(df_index['Scrape_Timestamp'])
    df_index = df_index.set_index('Scrape_Timestamp').sort_index()

    # Convert 'Last Price' to numeric, handling potential errors
    df_index[TARGET_COLUMN] = pd.to_numeric(df_index[TARGET_COLUMN], errors='coerce')
    df_index = df_index.dropna(subset=[TARGET_COLUMN])

    if df_index.empty:
        print(f"No valid numeric '{TARGET_COLUMN}' data after cleaning for {index_name}.")
        return pd.DataFrame()

    # Create lagged features (previous day's prices)
    for i in range(1, 6): # Lag by up to 5 days
        df_index[f'Lag_{i}_Price'] = df_index[TARGET_COLUMN].shift(i)

    # Create rolling window features (e.g., 5-day rolling mean)
    df_index['Rolling_Mean_5'] = df_index[TARGET_COLUMN].rolling(window=5).mean().shift(1)
    df_index['Rolling_Std_5'] = df_index[TARGET_COLUMN].rolling(window=5).std().shift(1)

    # Create time-based features
    df_index['Day_of_Week'] = df_index.index.dayofweek
    df_index['Day_of_Month'] = df_index.index.day
    df_index['Month'] = df_index.index.month
    df_index['Year'] = df_index.index.year

    # Drop rows with NaN values created by shifting/rolling (first few rows)
    df_index = df_index.dropna()

    print(f"Processed data for {index_name}. Shape: {df_index.shape}")
    return df_index

# --- Model Training Function ---
def train_models(df_processed):
    """
    Trains regression models (Ridge, GradientBoosting) and an ARIMA model.
    Also trains an Isolation Forest for anomaly detection.
    """
    if df_processed.empty:
        print("Processed DataFrame is empty. Cannot train models.")
        return None, None, None, None

    X = df_processed.drop(columns=[TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change'], errors='ignore')
    y = df_processed[TARGET_COLUMN]

    # Align X and y indices
    X, y = X.align(y, join='inner', axis=0)

    if X.empty or y.empty:
        print("Features or target are empty after alignment. Cannot train models.")
        return None, None, None, None

    # Split data for supervised learning models
    # Use a time-series split if possible, but for simplicity, a standard split for now.
    # For time series, it's better to train on older data and test on newer data.
    train_size = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

    print(f"Training data size: {len(X_train)} | Test data size: {len(X_test)}")

    # --- Ridge Regression ---
    ridge_model = Ridge(alpha=1.0)
    ridge_model.fit(X_train, y_train)
    ridge_preds = ridge_model.predict(X_test)
    print(f"\nRidge Regression - MSE: {mean_squared_error(y_test, ridge_preds):.2f}, R2: {r2_score(y_test, ridge_preds):.2f}")

    # --- Gradient Boosting Regressor (Ensemble Method) ---
    gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
    gb_model.fit(X_train, y_train)
    gb_preds = gb_model.predict(X_test)
    print(f"Gradient Boosting - MSE: {mean_squared_error(y_test, gb_preds):.2f}, R2: {r2_score(y_test, gb_preds):.2f}")

    # --- ARIMA Model ---
    # ARIMA requires a univariate series. We'll train it on the target column directly.
    # p, d, q parameters need to be tuned. (1,1,0) is a common starting point for non-seasonal.
    # d=1 means differencing once to make it stationary.
    try:
        arima_model_fit = ARIMA(y_train, order=(5,1,0)).fit() # Example order, needs tuning
        print(f"\nARIMA Model Summary:\n{arima_model_fit.summary()}")
        arima_preds = arima_model_fit.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, typ='levels')
        # Ensure arima_preds index aligns with y_test for metrics
        arima_preds.index = y_test.index
        print(f"ARIMA - MSE: {mean_squared_error(y_test, arima_preds):.2f}, R2: {r2_score(y_test, arima_preds):.2f}")
    except Exception as e:
        print(f"Error training or predicting with ARIMA: {e}")
        arima_model_fit = None # Set to None if training fails

    # --- Anomaly Detection (Isolation Forest) ---
    # Train Isolation Forest on the features (X_train)
    # contamination is the proportion of outliers in the data set.
    # Adjust this based on your expected anomaly rate.
    iso_forest = IsolationForest(contamination=0.01, random_state=42)
    iso_forest.fit(X_train) # Fit on training features

    print("\nModels trained successfully.")
    return ridge_model, gb_model, arima_model_fit, iso_forest

# --- Prediction Function ---
def predict_trends(ridge_model, gb_model, arima_model_fit, df_processed, latest_data_point):
    """
    Predicts future market trends for 24 hours and 1 month.
    """
    if ridge_model is None or gb_model is None or df_processed.empty:
        print("Models or processed data not available for prediction.")
        return None, None

    print("\n--- Making Future Predictions ---")

    # Prepare features for the next prediction point
    # This is crucial: we need to create the next X based on the latest available data.
    # This assumes the latest_data_point is a pandas Series or DataFrame row
    # with the same structure as a row in df_processed before dropping target.

    # Create a dummy DataFrame for the next prediction point
    # We need to manually construct the features for the next day/month based on the last known data.

    last_known_data = df_processed.iloc[-1]

    # Construct features for the next day (24 hours)
    # This requires careful construction of lagged features for the future point.
    # For simplicity, we'll shift the existing features.

    # For a real-world scenario, you'd calculate these based on the actual last known values
    # e.g., next_day_lag1 = last_known_data[TARGET_COLUMN]
    # next_day_lag2 = last_known_data['Lag_1_Price'] etc.

    future_features_24hr = {}
    for i in range(1, 5): # Lag by up to 4 days for the next day's prediction
        future_features_24hr[f'Lag_{i}_Price'] = last_known_data[f'Lag_{i-1}_Price'] if i > 1 else last_known_data[TARGET_COLUMN]
    future_features_24hr['Lag_5_Price'] = last_known_data['Lag_4_Price'] # For 5th lag

    future_features_24hr['Rolling_Mean_5'] = df_processed[TARGET_COLUMN].iloc[-5:].mean() # Mean of last 5 prices
    future_features_24hr['Rolling_Std_5'] = df_processed[TARGET_COLUMN].iloc[-5:].std() # Std of last 5 prices

    # Time-based features for the next day
    next_day_timestamp = last_known_data.name + datetime.timedelta(days=1)
    future_features_24hr['Day_of_Week'] = next_day_timestamp.dayofweek
    future_features_24hr['Day_of_Month'] = next_day_timestamp.day
    future_features_24hr['Month'] = next_day_timestamp.month
    future_features_24hr['Year'] = next_day_timestamp.year

    # Convert to DataFrame for prediction
    X_future_24hr = pd.DataFrame([future_features_24hr])

    # Ensure columns match training data
    training_cols = df_processed.drop(columns=[TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change'], errors='ignore').columns
    X_future_24hr = X_future_24hr.reindex(columns=training_cols, fill_value=0) # Fill missing with 0 or appropriate value

    # Predict for 24 hours
    ridge_pred_24hr = ridge_model.predict(X_future_24hr)[0]
    gb_pred_24hr = gb_model.predict(X_future_24hr)[0]

    arima_pred_24hr = None
    if arima_model_fit:
        try:
            # ARIMA predicts directly from the series, not features
            arima_pred_24hr = arima_model_fit.forecast(steps=PREDICTION_HORIZON_24HR)[0]
        except Exception as e:
            print(f"ARIMA 24hr prediction failed: {e}")

    print(f"Next 24 hours (1 day) predictions:")
    print(f"  Ridge: {ridge_pred_24hr:.2f}")
    print(f"  Gradient Boosting: {gb_pred_24hr:.2f}")
    if arima_pred_24hr is not None:
        print(f"  ARIMA: {arima_pred_24hr:.2f}")

    # --- Predict for 1 Month (30 days) ---
    # This is more challenging as features need to be generated iteratively or forecasted themselves.
    # For simplicity, we'll predict 30 steps ahead using ARIMA and average the regression predictions
    # over 30 iterative steps. This is a simplification and might not be robust.

    ridge_preds_month = []
    gb_preds_month = []
    current_features = X_future_24hr.copy()
    current_last_price = ridge_pred_24hr # Use ridge's 24hr prediction as the starting point for iteration

    for i in range(PREDICTION_HORIZON_MONTH):
        # Update lagged features for the next iteration
        for j in range(5, 1, -1): # Shift lags
            current_features[f'Lag_{j}_Price'] = current_features[f'Lag_{j-1}_Price']
        current_features['Lag_1_Price'] = current_last_price # The predicted price becomes the new lag_1

        # Update rolling features (simplified: use current_last_price for mean/std approximation)
        # For more accuracy, you'd need to maintain a rolling window of actual/predicted values.
        current_features['Rolling_Mean_5'] = (current_features['Rolling_Mean_5'] * 4 + current_last_price) / 5
        current_features['Rolling_Std_5'] = current_features['Rolling_Std_5'] # Keep same for simplicity, or re-calculate

        # Update time-based features
        current_timestamp = next_day_timestamp + datetime.timedelta(days=i)
        current_features['Day_of_Week'] = current_timestamp.dayofweek
        current_features['Day_of_Month'] = current_timestamp.day
        current_features['Month'] = current_timestamp.month
        current_features['Year'] = current_timestamp.year

        ridge_pred = ridge_model.predict(current_features)[0]
        gb_pred = gb_model.predict(current_features)[0]

        ridge_preds_month.append(ridge_pred)
        gb_preds_month.append(gb_pred)
        current_last_price = ridge_pred # Use one model's prediction to feed the next iteration

    avg_ridge_month_pred = np.mean(ridge_preds_month)
    avg_gb_month_pred = np.mean(gb_preds_month)

    arima_pred_month = None
    if arima_model_fit:
        try:
            # ARIMA can forecast multiple steps directly
            arima_forecast_series = arima_model_fit.forecast(steps=PREDICTION_HORIZON_MONTH)
            arima_pred_month = arima_forecast_series.iloc[-1] # Take the last predicted value for month-end
        except Exception as e:
            print(f"ARIMA 1-month prediction failed: {e}")

    print(f"\nNext 1 month ({PREDICTION_HORIZON_MONTH} days) predictions:")
    print(f"  Ridge (Avg): {avg_ridge_month_pred:.2f}")
    print(f"  Gradient Boosting (Avg): {avg_gb_month_pred:.2f}")
    if arima_pred_month is not None:
        print(f"  ARIMA (Month End): {arima_pred_month:.2f}")

    return {
        '24hr_ridge': ridge_pred_24hr,
        '24hr_gb': gb_pred_24hr,
        '24hr_arima': arima_pred_24hr,
        'month_ridge_avg': avg_ridge_month_pred,
        'month_gb_avg': avg_gb_month_pred,
        'month_arima_end': arima_pred_month
    }

# --- Anomaly Detection Function ---
def detect_anomalies(iso_forest_model, df_processed):
    """
    Detects anomalies in the processed data using the Isolation Forest model.
    """
    if iso_forest_model is None or df_processed.empty:
        print("Isolation Forest model or processed data not available for anomaly detection.")
        return pd.DataFrame()

    print("\n--- Detecting Anomalies ---")

    # Features used for anomaly detection should be the same as training
    X_anomaly = df_processed.drop(columns=[TARGET_COLUMN, 'Index Name', 'Change', 'Percent Change'], errors='ignore')

    # Predict anomalies (-1 for outlier, 1 for inlier)
    df_processed['anomaly_score'] = iso_forest_model.decision_function(X_anomaly)
    df_processed['is_anomaly'] = iso_forest_model.predict(X_anomaly)

    anomalies = df_processed[df_processed['is_anomaly'] == -1]

    if not anomalies.empty:
        print(f"Detected {len(anomalies)} anomalies:")
        print(anomalies[[TARGET_COLUMN, 'anomaly_score', 'is_anomaly']])
    else:
        print("No anomalies detected.")

    return anomalies

# --- Main Automation Function for Prediction and Anomaly Detection ---
def run_market_prediction_automation(index_to_analyze="NIFTY 50"):
    """
    Orchestrates the market trend prediction and anomaly detection process.
    """
    print(f"\n[{datetime.datetime.now()}] --- Starting Market Trend Prediction and Anomaly Detection ---")
    print(f"Analyzing index: {index_to_analyze}")

    # 1. Load historical data
    combined_data = load_historical_data(SCRAPED_DATA_DIR)
    if combined_data.empty:
        print(f"[{datetime.datetime.now()}] No historical data loaded. Exiting.")
        return

    # 2. Preprocess data and engineer features
    processed_data = preprocess_data(combined_data, index_name=index_to_analyze)
    if processed_data.empty:
        print(f"[{datetime.datetime.now()}] Data preprocessing failed or resulted in empty DataFrame. Exiting.")
        return

    # Ensure we have enough data for lags and training
    if len(processed_data) < 10: # Arbitrary minimum for meaningful features/training
        print(f"[{datetime.datetime.now()}] Not enough historical data ({len(processed_data)} records) for meaningful analysis. Need at least 10-15 records.")
        return

    # 3. Train models
    ridge_m, gb_m, arima_m, iso_f = train_models(processed_data)
    if ridge_m is None: # If any core model failed to train
        print(f"[{datetime.datetime.now()}] Model training failed. Exiting.")
        return

    # 4. Predict future trends
    # Pass the last data point from the processed data to help construct future features
    latest_data_point = processed_data.iloc[-1]
    predictions = predict_trends(ridge_m, gb_m, arima_m, processed_data, latest_data_point)

    # 5. Detect anomalies
    anomalies_df = detect_anomalies(iso_f, processed_data)

    print(f"\n[{datetime.datetime.now()}] --- Market Trend Prediction and Anomaly Detection Finished ---")

    # You can return predictions and anomalies for further use
    return predictions, anomalies_df

# --- Main execution block ---
if __name__ == "__main__":
    # Example usage:
    # Make sure you have scraped data in the 'nifty_daily_data_csv' directory first!
    # You can change the index_to_analyze if your data contains other indices.

    # To run the prediction and anomaly detection:
    predictions, anomalies = run_market_prediction_automation(index_to_analyze="NIFTY 50")

    if predictions:
        print("\nFinal Predictions Summary:")
        for key, value in predictions.items():
            if value is not None:
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: N/A (Model failed or not applicable)")

    if anomalies is not None and not anomalies.empty:
        print("\nDetected Anomalies (Full Details):")
        print(anomalies)