# -*- coding: utf-8 -*-
"""Company News Scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S1YhlxNw6xb67Q9oGQoLA5Z79rnzse2G
"""

import requests
from bs4 import BeautifulSoup
import datetime
import os
import json
import time # For polite delays

def scrape_company_news(company_name, search_query_base="news about", num_pages=1, output_directory="company_news"):

    print(f"\n[{datetime.datetime.now()}] --- Starting Company News Scraper for '{company_name}' ---")
    base_search_url = "https://www.example-news-site.com/search?q=" # Placeholder!

    # Encode the query for URL
    encoded_query = requests.utils.quote(f"{search_query_base} {company_name}")

    all_news_articles = []

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "Accept-Language": "en-US,en;q=0.9",
    }

    for page in range(num_pages):
        # This page parameter might vary for different sites (e.g., &start=10, &page=2)
        current_url = f"{base_search_url}{encoded_query}&page={page + 1}" # Placeholder for page param
        print(f"[{datetime.datetime.now()}] Scraping page {page + 1}: {current_url}")

        try:
            response = requests.get(current_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # --- IMPORTANT: These selectors are placeholders. You MUST inspect the target news site's HTML ---
            # Look for common patterns like:
            # <div class="news-article">
            #   <h2 class="article-title"><a href="...">Headline</a></h2>
            #   <p class="article-summary">Summary text...</p>
            #   <span class="article-date">Date</span>
            # </div>

            article_containers = soup.find_all('div', class_='news-article') # Placeholder class

            if not article_containers:
                print(f"[{datetime.datetime.now()}] No article containers found on page {page + 1}. Check selectors or page structure.")
                break # Stop if no articles are found

            for container in article_containers:
                title_elem = container.find('h2', class_='article-title') # Placeholder class
                link_elem = title_elem.find('a') if title_elem else None
                summary_elem = container.find('p', class_='article-summary') # Placeholder class
                date_elem = container.find('span', class_='article-date') # Placeholder class

                title = title_elem.text.strip() if title_elem else 'N/A'
                link = link_elem['href'] if link_elem and 'href' in link_elem.attrs else 'N/A'
                summary = summary_elem.text.strip() if summary_elem else 'N/A'
                published_date = date_elem.text.strip() if date_elem else 'N/A'

                all_news_articles.append({
                    'company': company_name,
                    'title': title,
                    'summary': summary,
                    'link': link,
                    'published_date': published_date,
                    'scrape_timestamp': datetime.datetime.now().isoformat()
                })

            time.sleep(2) # Be polite, add a delay between page requests

        except requests.exceptions.RequestException as e:
            print(f"[{datetime.datetime.now()}] Error fetching page {page + 1}: {e}")
            break
        except Exception as e:
            print(f"[{datetime.datetime.now()}] An error occurred while parsing page {page + 1}: {e}")
            break

    if all_news_articles:
        print(f"[{datetime.datetime.now()}] Scraped {len(all_news_articles)} news articles for '{company_name}'.")
        save_news_data(all_news_articles, output_directory, company_name)
    else:
        print(f"[{datetime.datetime.now()}] No news articles found for '{company_name}'.")

    print(f"[{datetime.datetime.now()}] --- Company News Scraper Finished ---")

def save_news_data(news_list, directory, company_name):
    """
    Saves the list of news articles to a local JSON file.
    """
    if not news_list:
        print(f"[{datetime.datetime.now()}] No news data to save for {company_name}.")
        return

    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"[{datetime.datetime.now()}] Created directory: {directory}")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{company_name.replace(' ', '_').lower()}_news_{timestamp}.json"
    file_path = os.path.join(directory, filename)

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=4)
        print(f"[{datetime.datetime.now()}] News data saved successfully to: {file_path}")
    except Exception as e:
        print(f"[{datetime.datetime.now()}] Error saving news data: {e}")

# --- Main execution block for news scraper ---
if __name__ == "__main__":
    # Example usage:
    # Replace "Hypothetical Company Inc." with the actual company you want to track.
    # Remember to adjust the `base_search_url` and HTML selectors within the function
    # to match the news website you intend to scrape.

    # For demonstration, this will likely not find anything unless you set up a mock site.
    # You need to replace the placeholder URL and selectors with real ones after inspection.
    scrape_company_news(company_name="Reliance Industries", num_pages=1)
    scrape_company_news(company_name="Tata Motors", num_pages=2, output_directory="tata_news")